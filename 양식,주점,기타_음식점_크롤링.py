# -*- coding: utf-8 -*-
"""양식,주점,기타 음식점 크롤링.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ACIyD-UP7w6OhtSAp0658fPDk3wzXblN
"""

import time
import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service

def crawl_reviews(url, driver_path):
    options = webdriver.ChromeOptions()
    options.add_argument('window-size=1920x1080')
    driver = webdriver.Chrome(service=Service(executable_path=driver_path), options=options)
    data_list = []

    try:
        driver.get(url)
        driver.implicitly_wait(10)

        # 상점명, 카테고리 가져오기
        store_name = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "span.GHAhO"))
        ).text if driver.find_elements(By.CSS_SELECTOR, "span.GHAhO") else "N/A"

        store_category = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "span.lnJFt"))
        ).text if driver.find_elements(By.CSS_SELECTOR, "span.lnJFt") else "N/A"

        # 더보기 버튼 클릭
        while True:
            try:
                more_button = driver.find_element(
                    By.XPATH,
                    '//*[@id="app-root"]/div/div/div/div[6]/div[2]/div[3]/div[2]/div/a'
                )
                if more_button.is_displayed():
                    more_button.click()
                    time.sleep(1.5)
                else:
                    break
            except (NoSuchElementException, ElementClickInterceptedException):
                break

        # 리뷰 데이터 추출
        soup = BeautifulSoup(driver.page_source, 'lxml')
        reviews = soup.select('a[data-pui-click-code="rvshowmore"]')
        for review in reviews:
            content = review.get_text(strip=True) or "내용 없음"
            data_list.append([store_name, store_category, content])

    finally:
        driver.quit()

    if data_list:
        return pd.DataFrame(data_list, columns=['store_name', 'store_category', 'content'])
    else:
        return None

if __name__ == "__main__":
    DRIVER_PATH = "C:\\Users\\rlaek\\OneDrive\\바탕 화면\\chromedriver-win64\\chromedriver.exe"

    urls = [
        # 양식
        "https://m.place.naver.com/restaurant/1956501321/review/visitor",
        "https://m.place.naver.com/restaurant/1616991942/review/visitor",
        "https://m.place.naver.com/restaurant/1154332788/review/visitor",
        "https://m.place.naver.com/restaurant/11641757/review/visitor",
        "https://m.place.naver.com/restaurant/35877450/review/visitor",
        "https://m.place.naver.com/restaurant/1291021648/review/visitor",
        "https://m.place.naver.com/restaurant/1670417542/review/visitor",
        "https://m.place.naver.com/restaurant/1589266080/review/visitor",
        "https://m.place.naver.com/restaurant/1957364765/review/visitor",
        "https://m.place.naver.com/restaurant/34308940/review/visitor",
        "https://m.place.naver.com/restaurant/1590698636/review/visitor",
        "https://m.place.naver.com/restaurant/38641900/review/visitor",
        # 기타
        "https://m.place.naver.com/restaurant/13584258/review/visitor",
        "https://m.place.naver.com/restaurant/405772075/review/visitor",
        "https://m.place.naver.com/restaurant/37801441/review/visitor",
        "https://m.place.naver.com/restaurant/1649335417/review/visitor",
        "https://m.place.naver.com/restaurant/1586942619/review/visitor",
        "https://m.place.naver.com/restaurant/32448341/review/visitor",
        "https://m.place.naver.com/restaurant/1445446311/review/visitor",
        "https://m.place.naver.com/restaurant/479542877/review/visitor",
        "https://m.place.naver.com/restaurant/36943345/review/visitor",
        "https://m.place.naver.com/restaurant/1200412430/review/visitor",
        "https://m.place.naver.com/restaurant/1928807931/review/visitor",
        "https://m.place.naver.com/restaurant/1681763622/review/visitor",
        "https://m.place.naver.com/restaurant/1287044284/review/visitor",
        "https://m.place.naver.com/restaurant/1633349278/review/visitor",
        "https://m.place.naver.com/restaurant/1084630927/review/visitor",
        # 주점
        "https://m.place.naver.com/restaurant/1577308017/review/visitor",
        "https://m.place.naver.com/restaurant/1852482619/review/visitor",
        "https://m.place.naver.com/restaurant/1260595489/review/visitor",
        "https://m.place.naver.com/restaurant/1797733874/review/visitor",
        "https://m.place.naver.com/restaurant/37209085/review/visitor",
        "https://m.place.naver.com/restaurant/37435056/review/visitor",
        "https://m.place.naver.com/restaurant/1221164706/review/visitor",
        "https://m.place.naver.com/restaurant/1624774875/review/visitor",
        "https://m.place.naver.com/restaurant/1866387087/review/visitor",
        "https://m.place.naver.com/restaurant/1354692206/review/visitor",
        "https://m.place.naver.com/restaurant/1627417321/review/visitor",
        "https://m.place.naver.com/restaurant/1732759418/review/visitor",
        "https://m.place.naver.com/restaurant/1725794491/review/visitor"
    ]

    all_dfs = []
    for url in urls:
        df = crawl_reviews(url, DRIVER_PATH)
        if df is not None:
            all_dfs.append(df)

    if all_dfs:
        result = pd.concat(all_dfs, ignore_index=True)
        result.to_csv('all_reviews.csv', index=False, encoding='utf-8-sig')
        print("all_reviews.csv 저장 완료")