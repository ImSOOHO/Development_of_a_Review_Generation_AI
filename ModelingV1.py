# -*- coding: utf-8 -*-
"""koGPT(contextual기반 증강방식).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nmxo4JW9zUNh2Fi9KsWz_RWk8mNHshV8

# 기본 세팅
"""

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')

from google.colab import drive
drive.mount('/content/drive')



"""# 데이터 불러오기"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/drive/MyDrive/포트폴리오 프로젝트/NLP 프로젝트/데이터/전처리V5(음식대분류추가).csv', encoding = 'utf-8-sig')

df

"""# 데이터 증강(문맥 기반 치환 -> 오버 샘플링 -> 단어 치환)"""

df

df['sentiment'].value_counts()

# 필요 라이브러리

!pip install googletrans==4.0.0-rc1
!pip install nlpaug
!pip install transformers
!pip install torch
!pip install scikit-learn

#import nlpaug.augmenter.word as naw
import pandas as pd
import random
from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline
import nltk
import pandas as pd
import numpy as np
import random
from sklearn.utils import resample
import torch
from transformers import pipeline

nltk.download('wordnet')

# KoBERT 모델 및 토크나이저 로드
model_name = "monologg/kobert-lm"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForMaskedLM.from_pretrained(model_name)
fill_mask = pipeline("fill-mask", model=model, tokenizer=tokenizer, top_k=5)

# 데이터 분리: 부정 리뷰 254, 긍정리뷰 54330 -> 부정 리뷰 증강 필요
neg_df = df[df['sentiment'] == 0].copy()
pos_df = df[df['sentiment'] == 2].copy()
print("부정 데이터 개수:", len(neg_df))
print("긍정 데이터 개수:", len(pos_df))

# 부정 리뷰별 단어 개수 계산
neg_df['word_count'] = neg_df['processed_review_final'].apply(lambda x: len(x.split()))
n_short = (neg_df['word_count'] <= 5).sum()    # 5단어 이하 리뷰
n_long  = (neg_df['word_count'] > 5).sum()      # 5단어 초과 리뷰
print("5단어 이하 리뷰:", n_short, "5단어 초과 리뷰:", n_long)

# 목표: 부정리뷰(254개) -> 문맥 기반 치환 방식 -> 리뷰 10000개 생성 -> 오버샘플링 진행 -> 약 50000개 부정리뷰 생성
# 5단어 이하 리뷰는 각 10회 증강, 나머지는 T회로 설정
T = 0
if n_long > 0:
    T = int(round((10000 - (n_short * 10)) / n_long))
    T = max(T, 1)
print("5단어 이하 리뷰 증강 횟수: 10회, 5단어 초과 리뷰 증강 횟수:", T)

# 문맥 기반 치환 함수
punctuations = set(['.', '!', '?', ',', 'ㆍ', '~', '…'])
stopwords_ko = set(['이', '가', '은', '는', '을', '를', '에', '에서', '에게', '한테',
                    '와', '과', '랑', '으로', '으로는', '만', '도', '까지', '까지는'])

def augment_review(sentence):
    """
    주어진 한국어 문장(sentence)에서 문맥에 맞게 최대 2개 단어를 치환하여 새로운 문장 생성.
    - 문장이 5단어 이하이면 1개, 5단어 초과이면 2개 단어 치환.
    - 토큰 리스트를 직접 수정하여 인덱스 오류를 최소화.
    """
    tokens = sentence.split()
    replace_count = 1 if len(tokens) <= 5 else 2

    # 치환 후보 인덱스 선정 -> 높은 인덱스부터 처리
    candidates = []
    for idx, word in enumerate(tokens):
        base = word.rstrip("".join(punctuations))
        if base and base not in stopwords_ko:
            candidates.append(idx)
    if not candidates:
        return sentence
    if replace_count > len(candidates):
        replace_count = len(candidates)
    target_indices = random.sample(candidates, replace_count)
    target_indices.sort(reverse=True)

    for idx in target_indices:
        word = tokens[idx]
        trailing = word[-1] if word and word[-1] in punctuations else ''
        base = word[:-1] if trailing else word
        tokens[idx] = "[MASK]" + trailing
        masked_sentence = " ".join(tokens)
        results = fill_mask(masked_sentence)
        if isinstance(results, dict):
            results = [results]
        filtered = [res for res in results if res['token_str'].strip() != base]
        if not filtered:
            chosen = results[0]
        else:
            top_n = 5 if len(filtered) >= 5 else len(filtered)
            chosen = random.choice(filtered[:top_n])
        tokens[idx] = chosen['token_str'].strip() + trailing
    return " ".join(tokens)

# 문맥 기반 증강
aug_texts = []
aug_stores = []

for idx, row in neg_df.iterrows():
    original = row['processed_review_final']
    n_trans = 10 if row['word_count'] <= 5 else T
    for i in range(n_trans):
        aug_text = augment_review(original)
        aug_texts.append(aug_text)
        aug_stores.append(row['store_name'])

print("생성된 증강 리뷰 개수:", len(aug_texts))

# 원본 부정 리뷰와 증강 리뷰 합치기
aug_neg_df = pd.DataFrame({
    'store_name': list(neg_df['store_name']) + aug_stores,
    'sentiment': [0]*(len(neg_df) + len(aug_texts)),
    'processed_review_final': list(neg_df['processed_review_final']) + aug_texts
})
print("전체 부정 리뷰 개수 (원본+증강):", aug_neg_df.shape[0])

# 오버샘플링 -> 부정 리뷰, 긍정 리뷰 수 맞추기
from sklearn.utils import resample
neg_balanced = resample(aug_neg_df, replace=True, n_samples=pos_df.shape[0], random_state=42)

# 최종 균형 데이터셋
balanced_df = pd.concat([pos_df, neg_balanced], ignore_index=True)
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
print("최종 균형 데이터셋 크기:", balanced_df.shape)

#!pip install datasets

# store_genre 결측치 보간

not_null_genre = balanced_df.dropna(subset=['store_genre'])
genre_map_df = not_null_genre.drop_duplicates(subset=['store_name'], keep='first')[['store_name', 'store_genre']]
genre_map = dict(zip(genre_map_df['store_name'], genre_map_df['store_genre']))

def fill_genre(row):
    if pd.isna(row['store_genre']):
        return genre_map.get(row['store_name'], None)  # 매핑이 없으면 None
    else:
        return row['store_genre']

balanced_df['store_genre'] = balanced_df.apply(fill_genre, axis=1)

balanced_df['store_genre'].isnull().sum()

balanced_df.head()

# store_category 결측치 보간
not_null_category = balanced_df.dropna(subset=['store_category'])
category_map_df = not_null_category.drop_duplicates(subset=['store_name'], keep='first')[['store_name', 'store_category']]
category_map = dict(zip(category_map_df['store_name'], category_map_df['store_category']))

def fill_category(row):
    if pd.isna(row['store_category']):
        # store_name에 대응하는 카테고리가 있으면 채워주고, 없으면 None
        return category_map.get(row['store_name'], None)
    else:
        return row['store_category']

balanced_df['store_category'] = balanced_df.apply(fill_category, axis=1)

balanced_df['store_category'].isnull().sum()

balanced_df.head()

#!pip install huggingface_hub

from datasets import Dataset
from huggingface_hub import login

# 허깅페이스 업로드
login(token=".")
dataset = Dataset.from_pandas(balanced_df)
dataset.push_to_hub("KingKDB/datasetV2")

balanced_df.to_csv('증강데이터찐.csv', encoding = 'utf-8-sig')

"""# koGPT2 모델링"""

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/포트폴리오 프로젝트/NLP 프로젝트/데이터/증강데이터찐.csv', encoding = 'utf-8-sig')

df = df.iloc[:,1:]

# 감정 텍스트 생성 (0=부정, 2=긍정)
df['sentiment_text'] = df['sentiment'].map({0: '부정', 2: '긍정'})

df.head()

# df.drop(['probability','sentiment'], axis=1).to_csv('NLP프로젝트증강데이터.csv', encoding = 'UTF-8-SIG', index=False)

# !pip install nlpaug transformers torch scikit-learn matplotlib seaborn bert-score konlpy sentence-transformers
# !pip install bert-score
# !pip install konlpy

import pandas as pd
import nltk
import re
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import random
from transformers import (GPT2LMHeadModel, AutoTokenizer, Trainer,
                          TrainingArguments, DataCollatorForLanguageModeling, pipeline)
from torch.utils.data import Dataset
from bert_score import score as bertscore_score
from konlpy.tag import Okt

# !pip install nlpaug transformers torch scikit-learn matplotlib seaborn bert-score konlpy sentence-transformers

okt = Okt()
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

# 결측치 처리 및 학습 칼럼(review_with_context) 생성
for col in ['content', 'content_clean', 'probability']:
    df[col] = df[col].fillna("").astype(str)

df['review_with_context'] = (df['store_name'] + " " +
                             df['store_genre'] + " " +
                             df['store_category'] + " " +
                             df['sentiment_text'] + " 리뷰: " +
                             df['processed_review_final'])


# koGPT-2 파인튜닝
model_name = "skt/kogpt2-base-v2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

class ReviewDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        if isinstance(text, list):
            text = " ".join(text)
        if not text.strip():
            text = " "
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        input_ids = encoding.input_ids.squeeze()
        attention_mask = encoding.attention_mask.squeeze()
        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": input_ids}

texts_full = df['review_with_context'].tolist()
dataset_full = ReviewDataset(texts_full, tokenizer, max_length=128)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# 학습 구조 설정
training_args = TrainingArguments(
    output_dir="./kogpt2_review_full",
    report_to="none",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=5e-5,
    warmup_steps=100,
    logging_steps=50,
    save_steps=200,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset_full,
)

print("전체 데이터셋으로 파인튜닝 시작")
trainer.train()

# 허깅페이스 모델 PUSH
from huggingface_hub import notebook_login
notebook_login()

model.save_pretrained("./saved_model")
tokenizer.save_pretrained("./saved_model")

model.push_to_hub("KingKDB/koGPTV3_contextbasedV4")
tokenizer.push_to_hub("KingKDB/koGPTV3_contextbasedV4")

"""# 리뷰 생성"""

df

"""df['review_with_context'] = (df['store_name'] + " " +
                             df['store_genre'] + " " +
                             df['store_category'] + " " +
                             df['sentiment_text'] + " 리뷰: " +
                             df['processed_review_final'])
"""

# !pip install deepmultilingualpunctuation

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import pandas as pd
from deepmultilingualpunctuation import PunctuationModel

# 모델 및 토크나이저 로드
model_name = "KingKDB/koGPTV3_contextbasedV4"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# deepmultilingualpunctuation 모델 초기화
punc_model = PunctuationModel()

# 프롬프트 생성 함수
def create_prompt(row, sentiment):
    return f"{row['store_name']} {row['store_genre']} {row['store_category']} {sentiment} 리뷰: "

# 후처리 함수
def complete_sentence(text):
    # 문장 부호 복원
    text_with_punc = punc_model.restore_punctuation(text)
    # 마지막 문장 부호 기준으로 자르기
    for punc in ['.', '!', '?']:
        if punc in text_with_punc:
            text_with_punc = text_with_punc[:text_with_punc.rfind(punc)+1]
            break
    return text_with_punc

unique_stores = df.drop_duplicates(subset=['store_name'])

# 프롬프트 지정 및 리뷰 생성
# 각 store마다 긍정 리뷰 2개, 부정 리뷰 2개씩 생성
generated_results = []
for _, row in unique_stores.iterrows():
    for sentiment in ["긍정", "부정"]:
        prompt = create_prompt(row, sentiment)
        generations = generator(
            prompt,
            max_length=100,
            num_return_sequences=2,
            do_sample=True,
            top_p=0.92,
            temperature=0.9,
            early_stopping=True,
            eos_token_id=tokenizer.eos_token_id
        )

        for gen in generations:
            raw_text = gen["generated_text"].replace(prompt, "").strip()
            finished_text = complete_sentence(raw_text)
            generated_results.append({
                "store_name": row['store_name'],
                "sentiment": sentiment,
                "prompt": prompt,
                "generated_review": finished_text
            })

# 결과 확인
generated_df = pd.DataFrame(generated_results)

# !pip install datasets

# 생성된 리뷰 데이터 허깅페이스 PUSH
from datasets import Dataset
from datasets import load_dataset

dataset = Dataset.from_pandas(generated_df)
dataset.push_to_hub("KingKDB/generated_resultsV4", token = '.')
dataset = load_dataset("KingKDB/generated_resultsV4")

