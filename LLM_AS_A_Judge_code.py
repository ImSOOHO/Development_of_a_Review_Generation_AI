# -*- coding: utf-8 -*-
"""NLP_í”„ë¡¬í”„íŠ¸ë§Œë“¤ê¸°.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Trmjbl4hatkANGUD5iw44xW4F2iUY8pg

# ê¸°ë³¸ ì„¸íŒ…
"""

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')

from google.colab import drive
drive.mount('/content/drive')

"""# í”„ë¡¬í”„íŠ¸ ë°ì´í„° í”„ë ˆì„ ë§Œë“¤ê¸°"""

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install -q huggingface_hub pandas

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from huggingface_hub import list_repo_files, hf_hub_download
import pandas as pd

# ë¦¬í¬ì§€í† ë¦¬ ì •ë³´
repo_id   = "KingKDB/generated_resultsV4"
repo_type = "dataset"  # <-- ìš”ì : repo_type="dataset" ìœ¼ë¡œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.

# íŒŒì¼ ëª©ë¡ ì¡°íšŒ
files = list_repo_files(repo_id=repo_id, repo_type=repo_type)
print("ğŸ“‚ Available files:", files)

# CSV ë˜ëŠ” Parquet íŒŒì¼ ìë™ ì„ íƒ
data_file = next(f for f in files if f.endswith((".csv", ".parquet")))
print(" Using file:", data_file)

# íŒŒì¼ ë‹¤ìš´ë¡œë“œ
local_path = hf_hub_download(
    repo_id=repo_id,
    repo_type=repo_type,
    filename=data_file
)

# pandasë¡œ ì½ê¸°
if data_file.endswith(".csv"):
    df = pd.read_csv(local_path)
else:
    df = pd.read_parquet(local_path)

print(df.shape)
df.head()

# ì»¬ëŸ¼ëª… ë³€ê²½
df.rename(columns={
    'prompt': 'Prompt',
    'generated_review': 'Generated Review'
}, inplace=True)

# ë³€ê²½ í™•ì¸
print(df.columns)
df.head()

df1 = df.iloc[:,2:]
df1.head()

df1['Prompt'].value_counts()

df_raw = pd.read_csv('/content/drive/MyDrive/á„‘á…©á„á…³á„‘á…©á†¯á„…á…µá„‹á…© á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/NLP á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„ƒá…¦á„‹á…µá„á…¥/á„Œá…¥á†«á„á…¥á„…á…µV5(á„‹á…³á†·á„‰á…µá†¨á„ƒá…¢á„‡á…®á†«á„…á…²á„á…®á„€á…¡).csv', encoding = 'utf-8-sig',index_col=0)

df_raw = df_raw.reset_index()

df_raw['keys'] = df_raw['store_name'] + ' ' + df_raw['store_genre']

df_raw

# keys ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ë³„ ìƒ˜í”Œë§
sampled_unique_df = (
    df_raw.drop_duplicates(subset=["store_genre", "keys"])
    .groupby("store_genre", group_keys=False)
    .apply(lambda x: x.sample(n=8, replace=False, random_state=42))
    .reset_index(drop=True)
)

df_prompt = sampled_unique_df.iloc[:,:3]

df_prompt

df_prompt = pd.concat([df_prompt] * 2, ignore_index=True)
df_prompt

# ê·¸ë£¹ë³„ ì¤‘ë³µ ì¸ë±ìŠ¤ ìƒì„± (0, 1)
dup_idx = df_prompt.groupby(
    ["store_name", "store_genre", "store_category"],
    group_keys=False
).cumcount()

# V1_prompt ë§Œë“¤ê¸°
# store_name store_genre store_category + " ê¸ì • ë¦¬ë·°" / " ë¶€ì • ë¦¬ë·°"
df_prompt["V1_prompt"] = (
    df_prompt["store_name"] + " " +
    df_prompt["store_genre"] + " " +
    df_prompt["store_category"] +
    dup_idx.map({0: " ê¸ì • ë¦¬ë·°", 1: " ë¶€ì • ë¦¬ë·°"})
)

# V2_prompt ë§Œë“¤ê¸°
# "[ê¸ì •]" or "[ë¶€ì •]" í† í° + store_name store_genre store_category + " ë¦¬ë·°"
df_prompt["V2_prompt"] = (
    dup_idx.map({0: "[ê¸ì •] ", 1: "[ë¶€ì •] "}) +
    df_prompt["store_name"] + " " +
    df_prompt["store_genre"] + " " +
    df_prompt["store_category"] + " ë¦¬ë·°"
)

df_prompt.to_csv('í”„ë¡¬í”„íŠ¸ë°ì´í„°í”„ë ˆì„.csv', encoding = 'utf-8-sig', index=False)

df_prompt

"""# V1 ë¦¬ë·° ìƒì„±"""

!pip install deepmultilingualpunctuation

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import pandas as pd
from deepmultilingualpunctuation import PunctuationModel

# ëª¨ë¸ ë¡œë“œ
model_name = "KingKDB/koGPTV3_contextbasedV4"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0  # GPU ì“°ì‹¤ ë•Œë§Œ, CPUë©´ ì§€ìš°ì„¸ìš”
)

# ë¬¸ì¥ ì™„ì„± í•¨ìˆ˜
punc_model = PunctuationModel()
def complete_sentence(text: str) -> str:
    txt = punc_model.restore_punctuation(text)
    for p in [".", "!", "?"]:
        if p in txt:
            return txt[: txt.rfind(p) + 1]
    return txt

# V1_prompt â†’ V1_generated ìƒì„±
results = []
for prompt in df_prompt["V1_prompt"]:
    out = generator(
        prompt,
        max_length=100,
        do_sample=True,
        top_p=0.92,
        temperature=0.9,
        eos_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )[0]["generated_text"]

    raw = out.replace(prompt, "").strip()
    fin = complete_sentence(raw)

    results.append({
        "V1_prompt": prompt,
        "V1_generated": fin
    })

# DataFrame ë³€í™˜
V1 = pd.DataFrame(results, columns=["V1_prompt", "V1_generated"])

V1

V1.to_csv('V1ë¦¬ë·°.csv', encoding = 'UTF-8-SIG', index = False)

"""# V2 ë¦¬ë·° ìƒì„±"""

!pip install -q transformers huggingface_hub kss

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ë¡œê·¸ì¸
import re
import torch
import pandas as pd
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
from kss import split_sentences

login(token="hf_")

# ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "heydabins/koGPT_finetuned_v2"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model     = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)
device    = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# í›„ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def clean_review(text: str) -> str:
    # ë°˜ë³µ ë¬¸ì ì¤„ì´ê¸°
    text = re.sub(r'(.)\1{3,}', r'\1\1', text)
    # í•œê¸€+ì˜ì–´ ë¶™ì€ ê²½ìš° ë¶„ë¦¬
    text = re.sub(r'([ê°€-í£])([A-Za-z])', r'\1 \2', text)
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = split_sentences(text)
    # ì¢…ê²° í‘œí˜„ íŒ¨í„´
    ending_patterns = (
        r'(ê°ì‚¬í•©ë‹ˆë‹¤|ë§›ìˆì–´ìš”|ë§›ìˆìŠµë‹ˆë‹¤|ì¶”ì²œí•©ë‹ˆë‹¤|ë˜ ì˜¬ê²Œìš”|ìµœê³ ì˜ˆìš”|ì˜ ë¨¹ì—ˆìŠµë‹ˆë‹¤|'
        r'ê² ìŠµë‹ˆë‹¤|ì—ˆìŠµë‹ˆë‹¤|ì•˜ìŒ|í• ê»˜ìš”|í• ê²Œìš”|ìŠµë‹ˆë‹¤|í•©ë‹ˆë‹¤|í•©ë‹ˆë‹¹|ì…ë‹ˆë‹¤|'
        r'ì˜ë¨¹ì—ˆìŠµë‹ˆë‹¤|ì¢‹ì•„ìš”|ì¢‹ì•„ìš©|ë°”ë¼ìš”|ë°”ë˜ìš”|í•´ì£¼ì…¨ì–´ìš”|ë“œì…¨ì–´ìš”|ë‹¤ë…€ì™”ì–´ìš”)(?=\s|$|[.!?])'
    )
    # ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ì‹œì‘ í‘œí˜„
    unnatural_starts = ['ìš”', 'ì„œ', 'ê³ ', 'ë°', 'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ', 'ë˜í•œ', 'ì•¼', 'í•  ì¼ì´ ìˆì–´']

    collected = []
    for s in sentences:
        # ë¶ˆí•„ìš” ì‹œì‘ì–´ ì œê±°
        for st in unnatural_starts:
            if s.strip().startswith(st):
                s = s.strip()[len(st):].lstrip()
                break
        collected.append(s.strip())
        if re.search(ending_patterns, s):
            break
    return " ".join(collected).strip()

# ìƒì„± í•¨ìˆ˜ ì •ì˜
def generate_review(prompt: str,
                    max_new_tokens: int = 150,
                    top_k: int = 50,
                    top_p: float = 0.95) -> str:
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    out = model.generate(
        input_ids,
        do_sample=True,
        max_new_tokens=max_new_tokens,
        top_k=top_k,
        top_p=top_p,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id
    )
    text = tokenizer.decode(out[0], skip_special_tokens=True)

    if text.startswith(prompt):
        text = text[len(prompt):].strip()
    return text

# V2 DataFrame ìƒì„±
generated = []
for prompt in df_prompt["V2_prompt"]:
    raw   = generate_review(prompt)
    clean = clean_review(raw)
    generated.append({
        "V2_prompt":    prompt,
        "V2_generated": clean
    })

V2 = pd.DataFrame(generated, columns=["V2_prompt", "V2_generated"])

# ê²°ê³¼ í™•ì¸
print(V2.shape)
V2.head()

V2["V2_generated"] = V2["V2_generated"].str.lstrip(": ").str.strip()

V2.to_csv('V2ë¦¬ë·°.csv', encoding = 'UTF-8-SIG', index = False)

"""# LLM-AS-A-JUDGE"""

import openai
import pandas as pd

# --- V2 ë¦¬ë·° LLM-as-a-Judge ---

# OpenAI API ì„¤ì •
client = openai.OpenAI(api_key="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")

# ë°ì´í„° ë¡œë“œ
csv_path = r"/content/drive/MyDrive/á„‘á…©á„á…³á„‘á…©á†¯á„…á…µá„‹á…© á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/NLP á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„ƒá…¦á„‹á…µá„á…¥/V2á„…á…µá„‡á…².csv"
df_input = pd.read_csv(csv_path, encoding='utf-8')

# ë£¨ë¸Œë¦­ ì •ì˜
rubrics = {
    "Grammaticality (ë¬¸ë²•ì„±)": """Criterion: Grammaticality (ë¬¸ë²•ì„±)
- Score 1: The text contains frequent grammatical errors, making it difficult to understand.
- Score 2: The text shows occasional grammatical errors, which disrupt the flow and clarity of the text.
- Score 3: The text generally adheres to grammatical rules, though minor errors are present.
- Score 4: The text demonstrates good grammaticality with rare errors that do not affect comprehension.
- Score 5: The text excels in grammatical usage, with clear and correct grammar throughout.""",

    "Fluency (ìœ ì°½ì„±)": """Criterion: Fluency (ìœ ì°½ì„±)
- Score 1: The text is disjointed and lacks fluency, making it hard to follow.
- Score 2: The text has limited fluency with frequent awkward phrasing.
- Score 3: The text is moderately fluent, with some awkward phrasing but generally easy to follow.
- Score 4: The text is fluent with smooth transitions and rare awkward phrases.
- Score 5: The text is highly fluent, with natural and smooth expression throughout.""",

    "Coherence (ì¼ê´€ì„±)": """Criterion: Coherence (ì¼ê´€ì„±)
- Score 1: The text is incoherent and lacks logical organization, making it difficult to understand.
- Score 2: The text shows some coherence but contains several disjointed ideas and poor organization.
- Score 3: The text is generally coherent with a logical flow, though minor lapses in organization may occur.
- Score 4: The text is coherent and well-organized with clear connections between ideas.
- Score 5: The text is highly coherent, with a strong logical structure and seamless organization.""",

    "Natural Sentiment (ìì—°ìŠ¤ëŸ¬ìš´ ê°ì„± í‘œí˜„)": """Criterion: Natural Sentiment (ìì—°ìŠ¤ëŸ¬ìš´ ê°ì„± í‘œí˜„)
- Score 1: The sentiment is robotic or forced, lacking authenticity.
- Score 2: Some emotional cues are present, but feel generic or unnatural.
- Score 3: The emotions are somewhat relatable, but still feel scripted.
- Score 4: The emotions are mostly natural and resemble real human reviews.
- Score 5: The emotional expressions are vivid, authentic, and resonate like a real human experience."""
}

rubric_criteria = list(rubrics.keys())

# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def make_prompt(criterion, prompt, review):
    return f"""
Please answer in Korean.

Evaluate the following AI-generated restaurant review based on the rubric below:

{rubrics[criterion]}

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Score: ?
"""

# ì¢…í•© í‰ê°€ í”„ë¡¬í”„íŠ¸
def make_summary_prompt(prompt, review):
    return f"""
Please answer in Korean.

You are an unbiased evaluator. Provide an overall evaluation of the following AI-generated restaurant review based on grammaticality, fluency, coherence, and natural emotional expression.

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Overall Comment: ?
"""

# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
results = []

# í‰ê°€ ë£¨í”„
for idx, row in df_input.iterrows():
    prompt = row["V2_prompt"]
    review = row["V2_generated"]
    scores = []

    # ê° ê¸°ì¤€ë³„ í‰ê°€
    for criterion in rubric_criteria:
        prompt_text = make_prompt(criterion, prompt, review)
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=[
                {"role": "system", "content": "You are an unbiased evaluator."},
                {"role": "user", "content": prompt_text}
            ]
        )
        content = response.choices[0].message.content.strip()
        score_line = content.split("\n")[0]
        score = int(score_line.split(":")[1].strip())
        scores.append(score)

    # ì¢…í•© ì´í‰
    summary_prompt = make_summary_prompt(prompt, review)
    summary_response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[
            {"role": "system", "content": "You are a strict and unbiased evaluator."},
            {"role": "user", "content": summary_prompt}
        ]
    )
    summary_text = summary_response.choices[0].message.content.strip()
    explanation = summary_text.split(":")[1].strip() if ":" in summary_text else summary_text

    results.append({
        "Prompt": prompt,
        "Generated Review": review,
        "Grammaticality": scores[0],
        "Fluency": scores[1],
        "Coherence": scores[2],
        "Natural Sentiment": scores[3],
        "Total Score": sum(scores),
        "Explanation": explanation
    })

# ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
df_output = pd.DataFrame(results)
print(df_output)

# total scoreì˜ í‰ê·  ê³„ì‚°

average_score = df_output["Total Score"].mean()
print(f"Total Score í‰ê· : {average_score}")

# CSV ì €ì¥
df_output.to_csv("LLM_AS_A_JUDGE_V2.csv", index=False, encoding="utf-8-sig")

# --- V1 ë¦¬ë·° LLM-as-a-Judge ---
import openai
import pandas as pd

client = openai.OpenAI(api_key="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")

# ë°ì´í„° ë¡œë“œ
csv_path = r"/content/drive/MyDrive/á„‘á…©á„á…³á„‘á…©á†¯á„…á…µá„‹á…© á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/NLP á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„ƒá…¦á„‹á…µá„á…¥/V1á„…á…µá„‡á…².csv"
df_input = pd.read_csv(csv_path, encoding='utf-8')

# ë£¨ë¸Œë¦­ ì •ì˜
rubrics = {
    "Grammaticality (ë¬¸ë²•ì„±)": """Criterion: Grammaticality (ë¬¸ë²•ì„±)
- Score 1: The text contains frequent grammatical errors, making it difficult to understand.
- Score 2: The text shows occasional grammatical errors, which disrupt the flow and clarity of the text.
- Score 3: The text generally adheres to grammatical rules, though minor errors are present.
- Score 4: The text demonstrates good grammaticality with rare errors that do not affect comprehension.
- Score 5: The text excels in grammatical usage, with clear and correct grammar throughout.""",

    "Fluency (ìœ ì°½ì„±)": """Criterion: Fluency (ìœ ì°½ì„±)
- Score 1: The text is disjointed and lacks fluency, making it hard to follow.
- Score 2: The text has limited fluency with frequent awkward phrasing.
- Score 3: The text is moderately fluent, with some awkward phrasing but generally easy to follow.
- Score 4: The text is fluent with smooth transitions and rare awkward phrases.
- Score 5: The text is highly fluent, with natural and smooth expression throughout.""",

    "Coherence (ì¼ê´€ì„±)": """Criterion: Coherence (ì¼ê´€ì„±)
- Score 1: The text is incoherent and lacks logical organization, making it difficult to understand.
- Score 2: The text shows some coherence but contains several disjointed ideas and poor organization.
- Score 3: The text is generally coherent with a logical flow, though minor lapses in organization may occur.
- Score 4: The text is coherent and well-organized with clear connections between ideas.
- Score 5: The text is highly coherent, with a strong logical structure and seamless organization.""",

    "Natural Sentiment (ìì—°ìŠ¤ëŸ¬ìš´ ê°ì„± í‘œí˜„)": """Criterion: Natural Sentiment (ìì—°ìŠ¤ëŸ¬ìš´ ê°ì„± í‘œí˜„)
- Score 1: The sentiment is robotic or forced, lacking authenticity.
- Score 2: Some emotional cues are present, but feel generic or unnatural.
- Score 3: The emotions are somewhat relatable, but still feel scripted.
- Score 4: The emotions are mostly natural and resemble real human reviews.
- Score 5: The emotional expressions are vivid, authentic, and resonate like a real human experience."""
}

rubric_criteria = list(rubrics.keys())

# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def make_prompt(criterion, prompt, review):
    return f"""
Please answer in Korean.

Evaluate the following AI-generated restaurant review based on the rubric below:

{rubrics[criterion]}

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Score: ?
"""

# ì¢…í•© í‰ê°€ í”„ë¡¬í”„íŠ¸
def make_summary_prompt(prompt, review):
    return f"""
Please answer in Korean.

You are an unbiased evaluator. Provide an overall evaluation of the following AI-generated restaurant review based on grammaticality, fluency, coherence, and natural emotional expression.

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Overall Comment: ?
"""

# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
results = []

# í‰ê°€ ë£¨í”„
for idx, row in df_input.iterrows():
    prompt = row["V1_prompt"]
    review = row["V1_generated"]
    scores = []

    # ê° ê¸°ì¤€ë³„ í‰ê°€
    for criterion in rubric_criteria:
        prompt_text = make_prompt(criterion, prompt, review)
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=[
                {"role": "system", "content": "You are an unbiased evaluator."},
                {"role": "user", "content": prompt_text}
            ]
        )
        content = response.choices[0].message.content.strip()
        score_line = content.split("\n")[0]
        score = int(score_line.split(":")[1].strip())
        scores.append(score)

    # ì¢…í•© ì´í‰
    summary_prompt = make_summary_prompt(prompt, review)
    summary_response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[
            {"role": "system", "content": "You are a strict and unbiased evaluator."},
            {"role": "user", "content": summary_prompt}
        ]
    )
    summary_text = summary_response.choices[0].message.content.strip()
    explanation = summary_text.split(":")[1].strip() if ":" in summary_text else summary_text

    results.append({
        "Prompt": prompt,
        "Generated Review": review,
        "Grammaticality": scores[0],
        "Fluency": scores[1],
        "Coherence": scores[2],
        "Natural Sentiment": scores[3],
        "Total Score": sum(scores),
        "Explanation": explanation
    })

# ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
df_output2 = pd.DataFrame(results)
print(df_output2)

# total scoreì˜ í‰ê·  ê³„ì‚°

average_score2 = df_output2["Total Score"].mean()
print(f"Total Score í‰ê· : {average_score2}")

# CSV ì €ì¥
df_output2.to_csv("LLM_AS_A_JUDGE_V1.csv", index=False, encoding="utf-8-sig")

"""# LLM-AS-A-JUDGE ì‹œê°í™”"""

import pandas as pd

df_v1 = pd.read_csv('/content/drive/MyDrive/á„‘á…©á„á…³á„‘á…©á†¯á„…á…µá„‹á…© á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/NLP á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„ƒá…¦á„‹á…µá„á…¥/LLM_AS_A_JUDGE_V1.csv', encoding = 'utf-8-sig', index_col = 0).reset_index()
df_v2 = pd.read_csv('/content/drive/MyDrive/á„‘á…©á„á…³á„‘á…©á†¯á„…á…µá„‹á…© á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/NLP á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„ƒá…¦á„‹á…µá„á…¥/LLM_AS_A_JUDGE_V2.csv', encoding = 'utf-8-sig', index_col = 0).reset_index()
print(df_v1.shape, df_v2.shape)

df_v1.head()

df_v2.head()

df_v1['keys'] = int(1)
df_v2['keys'] = int(2)

df = pd.concat([df_v1, df_v2], axis=0)

df.shape

df.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Seaborn
sns.set_theme(style="whitegrid", palette="Set2", font_scale=1.1)

# í‰ê°€ ì§€í‘œ ë¦¬ìŠ¤íŠ¸ ì •ì˜
metrics = ['Grammaticality', 'Fluency', 'Coherence', 'Natural Sentiment', 'Total Score']

df_melt = df.melt(
    id_vars='keys',
    value_vars=metrics,
    var_name='Metric',
    value_name='Score'
)

plt.figure(figsize=(14, 7))

# Barplot: keysë³„ ê° Metricì— ëŒ€í•œ í‰ê·  ë° í‘œì¤€í¸ì°¨ í‘œì‹œ
sns.barplot(
    data=df_melt,
    x='Metric',
    y='Score',
    hue='keys',
    ci='sd',
    capsize=0.15,
    palette=['#4C72B0', '#DD8452']
)

# ê°œë³„ ë°ì´í„° í¬ì¸íŠ¸ ë¶„í¬ë¥¼ ì‹œê°í™”
sns.stripplot(
    data=df_melt,
    x='Metric',
    y='Score',
    hue='keys',
    dodge=True,
    jitter=True,
    size=6,
    alpha=0.7,
    palette=['#4C72B0', '#DD8452']
)

# ë²”ë¡€ ì •ë¦¬
handles, labels = plt.gca().get_legend_handles_labels()
unique = dict(zip(labels[:2], handles[:2]))
plt.legend(
    unique.values(),
    unique.keys(),
    title="Key",
    loc='upper right',
    fontsize=12,
    title_fontsize=13,
    frameon=True,
    framealpha=0.8
)

# ì œëª©
plt.title("Comparison", fontsize=18, pad=15)
plt.xlabel("", fontsize=14)
plt.ylabel("Score", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

plt.tight_layout()
plt.show()

# ëª¨ë¸ë³„ í‰ê·  ê³„ì‚°
metrics = ['Grammaticality', 'Fluency', 'Coherence', 'Natural Sentiment', 'Total Score']
mean_table = df.groupby('keys')[metrics].mean().reset_index()

# ì°¨ì´(V2 âˆ’ V1) ê³„ì‚°
mean_key1 = mean_table[mean_table['keys'] == 1].set_index('keys')[metrics].iloc[0]
mean_key2 = mean_table[mean_table['keys'] == 2].set_index('keys')[metrics].iloc[0]
difference = (mean_key2 - mean_key1).rename('Difference')

result_df = pd.DataFrame({
    'Key=1 Mean': mean_key1,
    'Key=2 Mean': mean_key2,
    'Difference': difference
})

print(result_df)

# í‰ê·  ë³€í™”ëŸ‰ ë§‰ëŒ€ê·¸ë˜í”„
plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x=result_df.index,
    y='Difference',
    data=result_df.reset_index(),
    palette='RdYlGn',
    dodge=False
)
plt.axhline(0, color='gray', linestyle='--', linewidth=1)

# ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
for patch in ax.patches:
    height = patch.get_height()
    ax.text(
        patch.get_x() + patch.get_width() / 2,
        height + 0.1,
        f"{height:.2f}",
        ha='center',
        va='bottom',
        fontsize=11,
        color='black'
    )

plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel("")
plt.ylabel("Mean Difference", fontsize=14)
plt.title("Difference", fontsize=18, pad=15)
plt.tight_layout()
plt.show()