# -*- coding: utf-8 -*-
"""NLP_프롬프트만들기.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Trmjbl4hatkANGUD5iw44xW4F2iUY8pg

# 기본 세팅
"""

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')

from google.colab import drive
drive.mount('/content/drive')

"""# 프롬프트 데이터 프레임 만들기"""

# 필요한 패키지 설치
!pip install -q huggingface_hub pandas

# 라이브러리 임포트
from huggingface_hub import list_repo_files, hf_hub_download
import pandas as pd

# 리포지토리 정보
repo_id   = "KingKDB/generated_resultsV4"
repo_type = "dataset"  # <-- 요점: repo_type="dataset" 으로 지정해야 합니다.

# 파일 목록 조회
files = list_repo_files(repo_id=repo_id, repo_type=repo_type)
print("📂 Available files:", files)

# CSV 또는 Parquet 파일 자동 선택
data_file = next(f for f in files if f.endswith((".csv", ".parquet")))
print(" Using file:", data_file)

# 파일 다운로드
local_path = hf_hub_download(
    repo_id=repo_id,
    repo_type=repo_type,
    filename=data_file
)

# pandas로 읽기
if data_file.endswith(".csv"):
    df = pd.read_csv(local_path)
else:
    df = pd.read_parquet(local_path)

print(df.shape)
df.head()

# 컬럼명 변경
df.rename(columns={
    'prompt': 'Prompt',
    'generated_review': 'Generated Review'
}, inplace=True)

# 변경 확인
print(df.columns)
df.head()

df1 = df.iloc[:,2:]
df1.head()

df1['Prompt'].value_counts()

df_raw = pd.read_csv('/content/drive/MyDrive/포트폴리오 프로젝트/NLP 프로젝트/데이터/전처리V5(음식대분류추가).csv', encoding = 'utf-8-sig',index_col=0)

df_raw = df_raw.reset_index()

df_raw['keys'] = df_raw['store_name'] + ' ' + df_raw['store_genre']

df_raw

# keys 기준으로 그룹별 샘플링
sampled_unique_df = (
    df_raw.drop_duplicates(subset=["store_genre", "keys"])
    .groupby("store_genre", group_keys=False)
    .apply(lambda x: x.sample(n=8, replace=False, random_state=42))
    .reset_index(drop=True)
)

df_prompt = sampled_unique_df.iloc[:,:3]

df_prompt

df_prompt = pd.concat([df_prompt] * 2, ignore_index=True)
df_prompt

# 그룹별 중복 인덱스 생성 (0, 1)
dup_idx = df_prompt.groupby(
    ["store_name", "store_genre", "store_category"],
    group_keys=False
).cumcount()

# V1_prompt 만들기
# store_name store_genre store_category + " 긍정 리뷰" / " 부정 리뷰"
df_prompt["V1_prompt"] = (
    df_prompt["store_name"] + " " +
    df_prompt["store_genre"] + " " +
    df_prompt["store_category"] +
    dup_idx.map({0: " 긍정 리뷰", 1: " 부정 리뷰"})
)

# V2_prompt 만들기
# "[긍정]" or "[부정]" 토큰 + store_name store_genre store_category + " 리뷰"
df_prompt["V2_prompt"] = (
    dup_idx.map({0: "[긍정] ", 1: "[부정] "}) +
    df_prompt["store_name"] + " " +
    df_prompt["store_genre"] + " " +
    df_prompt["store_category"] + " 리뷰"
)

df_prompt.to_csv('프롬프트데이터프레임.csv', encoding = 'utf-8-sig', index=False)

df_prompt

"""# V1 리뷰 생성"""

!pip install deepmultilingualpunctuation

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import pandas as pd
from deepmultilingualpunctuation import PunctuationModel

# 모델 로드
model_name = "KingKDB/koGPTV3_contextbasedV4"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0  # GPU 쓰실 때만, CPU면 지우세요
)

# 문장 완성 함수
punc_model = PunctuationModel()
def complete_sentence(text: str) -> str:
    txt = punc_model.restore_punctuation(text)
    for p in [".", "!", "?"]:
        if p in txt:
            return txt[: txt.rfind(p) + 1]
    return txt

# V1_prompt → V1_generated 생성
results = []
for prompt in df_prompt["V1_prompt"]:
    out = generator(
        prompt,
        max_length=100,
        do_sample=True,
        top_p=0.92,
        temperature=0.9,
        eos_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )[0]["generated_text"]

    raw = out.replace(prompt, "").strip()
    fin = complete_sentence(raw)

    results.append({
        "V1_prompt": prompt,
        "V1_generated": fin
    })

# DataFrame 변환
V1 = pd.DataFrame(results, columns=["V1_prompt", "V1_generated"])

V1

V1.to_csv('V1리뷰.csv', encoding = 'UTF-8-SIG', index = False)

"""# V2 리뷰 생성"""

!pip install -q transformers huggingface_hub kss

# 라이브러리 임포트 및 로그인
import re
import torch
import pandas as pd
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
from kss import split_sentences

login(token="hf_")

# 모델, 토크나이저 로드
model_name = "heydabins/koGPT_finetuned_v2"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model     = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)
device    = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# 후처리 함수 정의
def clean_review(text: str) -> str:
    # 반복 문자 줄이기
    text = re.sub(r'(.)\1{3,}', r'\1\1', text)
    # 한글+영어 붙은 경우 분리
    text = re.sub(r'([가-힣])([A-Za-z])', r'\1 \2', text)
    # 문장 분리
    sentences = split_sentences(text)
    # 종결 표현 패턴
    ending_patterns = (
        r'(감사합니다|맛있어요|맛있습니다|추천합니다|또 올게요|최고예요|잘 먹었습니다|'
        r'겠습니다|었습니다|았음|할께요|할게요|습니다|합니다|합니당|입니다|'
        r'잘먹었습니다|좋아요|좋아용|바라요|바래요|해주셨어요|드셨어요|다녀왔어요)(?=\s|$|[.!?])'
    )
    # 부자연스러운 시작 표현
    unnatural_starts = ['요', '서', '고', '데', '그리고', '그래서', '하지만', '또한', '야', '할 일이 있어']

    collected = []
    for s in sentences:
        # 불필요 시작어 제거
        for st in unnatural_starts:
            if s.strip().startswith(st):
                s = s.strip()[len(st):].lstrip()
                break
        collected.append(s.strip())
        if re.search(ending_patterns, s):
            break
    return " ".join(collected).strip()

# 생성 함수 정의
def generate_review(prompt: str,
                    max_new_tokens: int = 150,
                    top_k: int = 50,
                    top_p: float = 0.95) -> str:
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    out = model.generate(
        input_ids,
        do_sample=True,
        max_new_tokens=max_new_tokens,
        top_k=top_k,
        top_p=top_p,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id
    )
    text = tokenizer.decode(out[0], skip_special_tokens=True)

    if text.startswith(prompt):
        text = text[len(prompt):].strip()
    return text

# V2 DataFrame 생성
generated = []
for prompt in df_prompt["V2_prompt"]:
    raw   = generate_review(prompt)
    clean = clean_review(raw)
    generated.append({
        "V2_prompt":    prompt,
        "V2_generated": clean
    })

V2 = pd.DataFrame(generated, columns=["V2_prompt", "V2_generated"])

# 결과 확인
print(V2.shape)
V2.head()

V2["V2_generated"] = V2["V2_generated"].str.lstrip(": ").str.strip()

V2.to_csv('V2리뷰.csv', encoding = 'UTF-8-SIG', index = False)

"""# LLM-AS-A-JUDGE"""

import openai
import pandas as pd

# --- V2 리뷰 LLM-as-a-Judge ---

# OpenAI API 설정
client = openai.OpenAI(api_key="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")

# 데이터 로드
csv_path = r"/content/drive/MyDrive/포트폴리오 프로젝트/NLP 프로젝트/데이터/V2리뷰.csv"
df_input = pd.read_csv(csv_path, encoding='utf-8')

# 루브릭 정의
rubrics = {
    "Grammaticality (문법성)": """Criterion: Grammaticality (문법성)
- Score 1: The text contains frequent grammatical errors, making it difficult to understand.
- Score 2: The text shows occasional grammatical errors, which disrupt the flow and clarity of the text.
- Score 3: The text generally adheres to grammatical rules, though minor errors are present.
- Score 4: The text demonstrates good grammaticality with rare errors that do not affect comprehension.
- Score 5: The text excels in grammatical usage, with clear and correct grammar throughout.""",

    "Fluency (유창성)": """Criterion: Fluency (유창성)
- Score 1: The text is disjointed and lacks fluency, making it hard to follow.
- Score 2: The text has limited fluency with frequent awkward phrasing.
- Score 3: The text is moderately fluent, with some awkward phrasing but generally easy to follow.
- Score 4: The text is fluent with smooth transitions and rare awkward phrases.
- Score 5: The text is highly fluent, with natural and smooth expression throughout.""",

    "Coherence (일관성)": """Criterion: Coherence (일관성)
- Score 1: The text is incoherent and lacks logical organization, making it difficult to understand.
- Score 2: The text shows some coherence but contains several disjointed ideas and poor organization.
- Score 3: The text is generally coherent with a logical flow, though minor lapses in organization may occur.
- Score 4: The text is coherent and well-organized with clear connections between ideas.
- Score 5: The text is highly coherent, with a strong logical structure and seamless organization.""",

    "Natural Sentiment (자연스러운 감성 표현)": """Criterion: Natural Sentiment (자연스러운 감성 표현)
- Score 1: The sentiment is robotic or forced, lacking authenticity.
- Score 2: Some emotional cues are present, but feel generic or unnatural.
- Score 3: The emotions are somewhat relatable, but still feel scripted.
- Score 4: The emotions are mostly natural and resemble real human reviews.
- Score 5: The emotional expressions are vivid, authentic, and resonate like a real human experience."""
}

rubric_criteria = list(rubrics.keys())

# 프롬프트 생성 함수
def make_prompt(criterion, prompt, review):
    return f"""
Please answer in Korean.

Evaluate the following AI-generated restaurant review based on the rubric below:

{rubrics[criterion]}

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Score: ?
"""

# 종합 평가 프롬프트
def make_summary_prompt(prompt, review):
    return f"""
Please answer in Korean.

You are an unbiased evaluator. Provide an overall evaluation of the following AI-generated restaurant review based on grammaticality, fluency, coherence, and natural emotional expression.

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Overall Comment: ?
"""

# 결과 저장 리스트
results = []

# 평가 루프
for idx, row in df_input.iterrows():
    prompt = row["V2_prompt"]
    review = row["V2_generated"]
    scores = []

    # 각 기준별 평가
    for criterion in rubric_criteria:
        prompt_text = make_prompt(criterion, prompt, review)
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=[
                {"role": "system", "content": "You are an unbiased evaluator."},
                {"role": "user", "content": prompt_text}
            ]
        )
        content = response.choices[0].message.content.strip()
        score_line = content.split("\n")[0]
        score = int(score_line.split(":")[1].strip())
        scores.append(score)

    # 종합 총평
    summary_prompt = make_summary_prompt(prompt, review)
    summary_response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[
            {"role": "system", "content": "You are a strict and unbiased evaluator."},
            {"role": "user", "content": summary_prompt}
        ]
    )
    summary_text = summary_response.choices[0].message.content.strip()
    explanation = summary_text.split(":")[1].strip() if ":" in summary_text else summary_text

    results.append({
        "Prompt": prompt,
        "Generated Review": review,
        "Grammaticality": scores[0],
        "Fluency": scores[1],
        "Coherence": scores[2],
        "Natural Sentiment": scores[3],
        "Total Score": sum(scores),
        "Explanation": explanation
    })

# 결과 저장 및 출력
df_output = pd.DataFrame(results)
print(df_output)

# total score의 평균 계산

average_score = df_output["Total Score"].mean()
print(f"Total Score 평균: {average_score}")

# CSV 저장
df_output.to_csv("LLM_AS_A_JUDGE_V2.csv", index=False, encoding="utf-8-sig")

# --- V1 리뷰 LLM-as-a-Judge ---
import openai
import pandas as pd

client = openai.OpenAI(api_key="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")

# 데이터 로드
csv_path = r"/content/drive/MyDrive/포트폴리오 프로젝트/NLP 프로젝트/데이터/V1리뷰.csv"
df_input = pd.read_csv(csv_path, encoding='utf-8')

# 루브릭 정의
rubrics = {
    "Grammaticality (문법성)": """Criterion: Grammaticality (문법성)
- Score 1: The text contains frequent grammatical errors, making it difficult to understand.
- Score 2: The text shows occasional grammatical errors, which disrupt the flow and clarity of the text.
- Score 3: The text generally adheres to grammatical rules, though minor errors are present.
- Score 4: The text demonstrates good grammaticality with rare errors that do not affect comprehension.
- Score 5: The text excels in grammatical usage, with clear and correct grammar throughout.""",

    "Fluency (유창성)": """Criterion: Fluency (유창성)
- Score 1: The text is disjointed and lacks fluency, making it hard to follow.
- Score 2: The text has limited fluency with frequent awkward phrasing.
- Score 3: The text is moderately fluent, with some awkward phrasing but generally easy to follow.
- Score 4: The text is fluent with smooth transitions and rare awkward phrases.
- Score 5: The text is highly fluent, with natural and smooth expression throughout.""",

    "Coherence (일관성)": """Criterion: Coherence (일관성)
- Score 1: The text is incoherent and lacks logical organization, making it difficult to understand.
- Score 2: The text shows some coherence but contains several disjointed ideas and poor organization.
- Score 3: The text is generally coherent with a logical flow, though minor lapses in organization may occur.
- Score 4: The text is coherent and well-organized with clear connections between ideas.
- Score 5: The text is highly coherent, with a strong logical structure and seamless organization.""",

    "Natural Sentiment (자연스러운 감성 표현)": """Criterion: Natural Sentiment (자연스러운 감성 표현)
- Score 1: The sentiment is robotic or forced, lacking authenticity.
- Score 2: Some emotional cues are present, but feel generic or unnatural.
- Score 3: The emotions are somewhat relatable, but still feel scripted.
- Score 4: The emotions are mostly natural and resemble real human reviews.
- Score 5: The emotional expressions are vivid, authentic, and resonate like a real human experience."""
}

rubric_criteria = list(rubrics.keys())

# 프롬프트 생성 함수
def make_prompt(criterion, prompt, review):
    return f"""
Please answer in Korean.

Evaluate the following AI-generated restaurant review based on the rubric below:

{rubrics[criterion]}

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Score: ?
"""

# 종합 평가 프롬프트
def make_summary_prompt(prompt, review):
    return f"""
Please answer in Korean.

You are an unbiased evaluator. Provide an overall evaluation of the following AI-generated restaurant review based on grammaticality, fluency, coherence, and natural emotional expression.

Prompt: "{prompt}"
Review: "{review}"

Return the result as:
- Overall Comment: ?
"""

# 결과 저장 리스트
results = []

# 평가 루프
for idx, row in df_input.iterrows():
    prompt = row["V1_prompt"]
    review = row["V1_generated"]
    scores = []

    # 각 기준별 평가
    for criterion in rubric_criteria:
        prompt_text = make_prompt(criterion, prompt, review)
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=[
                {"role": "system", "content": "You are an unbiased evaluator."},
                {"role": "user", "content": prompt_text}
            ]
        )
        content = response.choices[0].message.content.strip()
        score_line = content.split("\n")[0]
        score = int(score_line.split(":")[1].strip())
        scores.append(score)

    # 종합 총평
    summary_prompt = make_summary_prompt(prompt, review)
    summary_response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[
            {"role": "system", "content": "You are a strict and unbiased evaluator."},
            {"role": "user", "content": summary_prompt}
        ]
    )
    summary_text = summary_response.choices[0].message.content.strip()
    explanation = summary_text.split(":")[1].strip() if ":" in summary_text else summary_text

    results.append({
        "Prompt": prompt,
        "Generated Review": review,
        "Grammaticality": scores[0],
        "Fluency": scores[1],
        "Coherence": scores[2],
        "Natural Sentiment": scores[3],
        "Total Score": sum(scores),
        "Explanation": explanation
    })

# 결과 저장 및 출력
df_output2 = pd.DataFrame(results)
print(df_output2)

# total score의 평균 계산

average_score2 = df_output2["Total Score"].mean()
print(f"Total Score 평균: {average_score2}")

# CSV 저장
df_output2.to_csv("LLM_AS_A_JUDGE_V1.csv", index=False, encoding="utf-8-sig")

"""# LLM-AS-A-JUDGE 시각화"""

import pandas as pd

df_v1 = pd.read_csv('/content/drive/MyDrive/포트폴리오 프로젝트/NLP 프로젝트/데이터/LLM_AS_A_JUDGE_V1.csv', encoding = 'utf-8-sig', index_col = 0).reset_index()
df_v2 = pd.read_csv('/content/drive/MyDrive/포트폴리오 프로젝트/NLP 프로젝트/데이터/LLM_AS_A_JUDGE_V2.csv', encoding = 'utf-8-sig', index_col = 0).reset_index()
print(df_v1.shape, df_v2.shape)

df_v1.head()

df_v2.head()

df_v1['keys'] = int(1)
df_v2['keys'] = int(2)

df = pd.concat([df_v1, df_v2], axis=0)

df.shape

df.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Seaborn
sns.set_theme(style="whitegrid", palette="Set2", font_scale=1.1)

# 평가 지표 리스트 정의
metrics = ['Grammaticality', 'Fluency', 'Coherence', 'Natural Sentiment', 'Total Score']

df_melt = df.melt(
    id_vars='keys',
    value_vars=metrics,
    var_name='Metric',
    value_name='Score'
)

plt.figure(figsize=(14, 7))

# Barplot: keys별 각 Metric에 대한 평균 및 표준편차 표시
sns.barplot(
    data=df_melt,
    x='Metric',
    y='Score',
    hue='keys',
    ci='sd',
    capsize=0.15,
    palette=['#4C72B0', '#DD8452']
)

# 개별 데이터 포인트 분포를 시각화
sns.stripplot(
    data=df_melt,
    x='Metric',
    y='Score',
    hue='keys',
    dodge=True,
    jitter=True,
    size=6,
    alpha=0.7,
    palette=['#4C72B0', '#DD8452']
)

# 범례 정리
handles, labels = plt.gca().get_legend_handles_labels()
unique = dict(zip(labels[:2], handles[:2]))
plt.legend(
    unique.values(),
    unique.keys(),
    title="Key",
    loc='upper right',
    fontsize=12,
    title_fontsize=13,
    frameon=True,
    framealpha=0.8
)

# 제목
plt.title("Comparison", fontsize=18, pad=15)
plt.xlabel("", fontsize=14)
plt.ylabel("Score", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

plt.tight_layout()
plt.show()

# 모델별 평균 계산
metrics = ['Grammaticality', 'Fluency', 'Coherence', 'Natural Sentiment', 'Total Score']
mean_table = df.groupby('keys')[metrics].mean().reset_index()

# 차이(V2 − V1) 계산
mean_key1 = mean_table[mean_table['keys'] == 1].set_index('keys')[metrics].iloc[0]
mean_key2 = mean_table[mean_table['keys'] == 2].set_index('keys')[metrics].iloc[0]
difference = (mean_key2 - mean_key1).rename('Difference')

result_df = pd.DataFrame({
    'Key=1 Mean': mean_key1,
    'Key=2 Mean': mean_key2,
    'Difference': difference
})

print(result_df)

# 평균 변화량 막대그래프
plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x=result_df.index,
    y='Difference',
    data=result_df.reset_index(),
    palette='RdYlGn',
    dodge=False
)
plt.axhline(0, color='gray', linestyle='--', linewidth=1)

# 막대 위에 값 표시
for patch in ax.patches:
    height = patch.get_height()
    ax.text(
        patch.get_x() + patch.get_width() / 2,
        height + 0.1,
        f"{height:.2f}",
        ha='center',
        va='bottom',
        fontsize=11,
        color='black'
    )

plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel("")
plt.ylabel("Mean Difference", fontsize=14)
plt.title("Difference", fontsize=18, pad=15)
plt.tight_layout()
plt.show()